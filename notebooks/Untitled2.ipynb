{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c8d9d4-1c4e-4be4-b07c-b51f3356d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import gpytorch\n",
    "import torch\n",
    "from gpytorch.kernels import Kernel\n",
    "from numpy import ndarray\n",
    "import os\n",
    "import math\n",
    "from math import floor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14fca62b-9d12-443f-a360-236f1a2ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b72dce32-4bdd-437f-ac13-c8f6124a9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenSquaredExponetialKernel(Kernel):\n",
    "    r\"\"\"\n",
    "      Computes a covariance matrix based on the generalized squared exponential kernel\n",
    "        between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
    "        .. math::\n",
    "          \\begin{equation*}\n",
    "              k_{(\\mathbf{x_1}, \\mathbf{x_2}) = \\sigma(x_1) sigma(x_2) \\sqrt \\left(\\fract{2 l(x_1) l(x_2)}{l(x_1)^2 + l(x_2)^2} \\right)\n",
    "                \\exp \\left( - \\fract{(x_1-x_2)^2}{l(x_1)^2 + l(x_2)^2} \\right)  \n",
    "          \\end{equation*}\n",
    "      where\n",
    "      \n",
    "      * :math:`\\sigma` is the signal variance\n",
    "        :math:`x_1` and :math:`x_2` scaled by the :attr:`lengthscale` parameter :math:`l`.\n",
    "    \"\"\"\n",
    "    has_lengthscale = False\n",
    "\n",
    "    def __init__(self, s1, s2, l1 , l2, log_transformed = False, **kwargs):\n",
    "        \n",
    "        if isinstance(s1, ndarray):\n",
    "            self.s1 = torch.tensor(s1, dtype=torch.float)\n",
    "        elif torch.is_tensor(s1):\n",
    "            self.s1 = s1\n",
    "        else:\n",
    "            raise TypeError(f\"variance is expected to be an array or tensor. \"\n",
    "                            f\"Got {type(s1)} instead.\")\n",
    "        if isinstance(s2, ndarray):\n",
    "            self.s2 = torch.tensor(s2, dtype=torch.float)\n",
    "        elif torch.is_tensor(s2):\n",
    "            self.s2 = s2\n",
    "        else:\n",
    "            raise TypeError(f\"variance is expected to be an array or tensor. \"\n",
    "                            f\"Got {type(s2)} instead.\")\n",
    "            \n",
    "        if isinstance(l1, ndarray):\n",
    "            self.l1 = torch.tensor(l1, dtype=torch.float)\n",
    "        elif torch.is_tensor(l):\n",
    "            self.l1 = l1\n",
    "        else:\n",
    "            raise TypeError(f\"lengthscale is expected to be an array or tensor. \"\n",
    "                            f\"Got {type(l1)} instead.\")\n",
    "\n",
    "        if isinstance(l2, ndarray):\n",
    "            self.l2 = torch.tensor(l2, dtype=torch.float)\n",
    "        elif torch.is_tensor(l):\n",
    "            self.l2 = l2\n",
    "        else:\n",
    "            raise TypeError(f\"lengthscale is expected to be an array or tensor. \"\n",
    "                            f\"Got {type(l2)} instead.\")\n",
    "\n",
    "\n",
    "        if log_transformed:\n",
    "          self.l1 = torch.exp(self.l1).double()\n",
    "          self.l2 = torch.exp(self.l2).double()\n",
    "          self.s1 = torch.exp(self.s1).double()\n",
    "          self.s2 = torch.exp(self.s2).double()\n",
    "        super(GenSquaredExponetialKernel, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        global shape\n",
    "        \n",
    "        n_features = x1.size()[1]\n",
    "\n",
    "        first_comp = 1\n",
    "        second_comp = 0\n",
    "        for i in range(n_features):\n",
    "          s = torch.pow(self.l1[i].view(-1,1), 2) + torch.pow(self.l2[i].view(1,-1), 2)\n",
    "          first_comp *= torch.sqrt(torch.div(2*self.l1[i].view(-1,1)*self.l2[i].view(1,-1), s))\n",
    "          dist = self.covar_dist(x1[:,i:i+1], x2[:,i:i+1], square_dist=True, diag=diag, **params)\n",
    "  \n",
    "          f = dist.div(s)\n",
    "          second_comp += f   \n",
    "        \n",
    "        first_comp *= torch.matmul(self.s1, self.s2.t())\n",
    "        second_comp = torch.exp(-second_comp)\n",
    "        k = first_comp * second_comp\n",
    "        if diag:\n",
    "            k = k[0]\n",
    "\n",
    "        return k\n",
    "    \n",
    "class MyGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, s1, s2, l1, l2, train_x, train_y, likelihood):\n",
    "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.set_train_data(train_x, train_y)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(GenSquaredExponetialKernel(s1, s2, l1, l2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75dc2316-bd91-4336-801c-39d42a21f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7755e2ba-5450-4127-8c08-0e8031659147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### Prepare fake data ########\n",
    "X = torch.randn(1000,4)[:,:-1]\n",
    "y = X[:,-1]\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :]\n",
    "train_y = y[:train_n]\n",
    "\n",
    "test_x = X[train_n:, :]\n",
    "test_y = y[train_n:]\n",
    "\n",
    "s1 = np.array([0.1]*3)\n",
    "s2 = np.array([0.2]*3)\n",
    "l1 = np.array([3.0]*3)\n",
    "l2 = np.array([4.0]*3)\n",
    "\n",
    "model = MyGPModel(s1, s2, l1, l2, train_x=train_x, train_y=train_y, likelihood=likelihood)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     train_x = train_x.cuda()\n",
    "#     train_y = train_y.cuda()\n",
    "#     model = model.cuda()\n",
    "#     likelihood = likelihood.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52c853f7-91ce-457d-b2f3-8842adf309e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = GenSquaredExponetialKernel(s1, s2, l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3605e51e-3e3c-44ec-a5b5-ab5eb0dc78bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor at 0x2af49780eb50>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ker(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85153d39-e332-4bc4-bfc0-b63f8e7ce9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564,\n",
       "        0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564, 0.0564])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ker(train_x).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9ebb7f9-de64-4eb1-96a2-3cbe3d4d7b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10 - Loss: 0.503\n",
      "Iter 2/10 - Loss: 0.455\n",
      "Iter 3/10 - Loss: 0.408\n",
      "Iter 4/10 - Loss: 0.361\n",
      "Iter 5/10 - Loss: 0.314\n",
      "Iter 6/10 - Loss: 0.266\n",
      "Iter 7/10 - Loss: 0.219\n",
      "Iter 8/10 - Loss: 0.172\n",
      "Iter 9/10 - Loss: 0.125\n",
      "Iter 10/10 - Loss: 0.078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "################################# Training step #############################\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 10\n",
    "# Find optimal model hyperparameters\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Loss func for GP, the log marginal likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "#train_x, train_y = train_x.type(torch.DoubleTensor), train_y.type(torch.DoubleTensor)\n",
    "for i in range(training_iter):\n",
    "  # Zero gradient from previous iteration\n",
    "  optimizer.zero_grad()\n",
    "  output = model(train_x)\n",
    "\n",
    "  # Calculate backprop gradients\n",
    "  loss = -mll(output, train_y)\n",
    "  loss.backward()\n",
    "\n",
    "  print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "  optimizer.step()\n",
    "\n",
    "########################## Make predictions ###############################\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "\n",
    "test_results_gpytorch = np.median((test_y - mean) / test_y, axis=0)\n",
    "test_results_gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73435f85-d48c-408d-b71f-d2d0285b94d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
