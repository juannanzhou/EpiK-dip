{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8fa84-c71b-493a-9c8d-4fa4b0829463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4ba6c4-554a-499a-9a57-831ce9fef021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def rl():\n",
    "    importlib.reload(Di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db4d4b5-112f-4432-b9e2-dfa948d538bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 |  0% |  0% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n",
      "|  4 |  0% |  0% |\n",
      "|  5 |  0% |  0% |\n",
      "|  6 |  0% |  0% |\n",
      "|  7 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ec58e7-4447-4f3e-bf5f-ed51affca87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1602e4a-ec13-4fc3-b189-b3b1cf73e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.special import binom as binom\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import Di\n",
    "\n",
    "output_device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7795813-1684-49a8-b283-21acdb3a096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f309085-b33f-4f85-bef7-26b8f811d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geno_file_list = []\n",
    "for path, currentDirectory, files in os.walk(\"96ghpptzvf-4/SData2/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\"geno.txt\"):\n",
    "            geno_file_list.append(file)\n",
    "\n",
    "geno_file_list = list(set(geno_file_list))\n",
    "\n",
    "env_list = [file.split('_')[0] for file in geno_file_list]\n",
    "\n",
    "env_list = sorted(env_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a217d511-3dfc-4ec0-b62a-bf7f2e81c1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaCl\n"
     ]
    }
   ],
   "source": [
    "env = env_list[5]\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232f0e16-2fab-476c-a9ce-af777f066603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"96ghpptzvf-4/SData2/\"+ env + \"_geno.txt\", sep='\\t', nrows=5, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9b3d44-db85-47bd-850b-d4b4160be00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(df.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea5557b-a26d-453d-aae4-e45be587f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NaCl_matsui_geno_t.pt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env + '_matsui_geno_t.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0b5530-ad06-4a8f-9424-d5117a2eedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/42117759/ipykernel_42460/1650851562.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  geno_t = torch.tensor(geno_t, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "geno_t = torch.load(env + '_matsui_geno_t.pt')\n",
    "\n",
    "geno_t = torch.tensor(geno_t, dtype=torch.float)\n",
    "\n",
    "geno_t = torch.transpose(geno_t, 0, 1)\n",
    "N, L = geno_t.shape\n",
    "\n",
    "# geno_t = geno_t[:, :5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5953bc-1441-4b15-9047-bc92c8153557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.read_csv(\"96ghpptzvf-4/SData6/\" + env + \"_pheno.txt\", sep='\\t', engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd03651-f9ce-47a9-bb6a-a8375ea5d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pheno.set_index('geno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e4fc78-4a7f-457e-a57f-57a3d677f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pheno.loc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f19cac8-653a-4206-a887-14376ee31dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pheno.pheno)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfff37-e18c-4095-933f-92e27ee7e05f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d16029-a360-4b5f-aa0d-01fd19adfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_sub = np.where(np.array(pheno.pheno < -0.6) == False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0be55d-c922-40f3-babe-ec6c2892dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bfd463-8723-4d71-b6b2-4b33ad425a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = np.random.choice(inds_sub, 100000)\n",
    "\n",
    "sub_t = np.random.choice(list(set(inds_sub).difference(sub)), 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f5d3767-89f5-4e30-a005-8ca73d024960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = geno_t[sub]\n",
    "train_y = torch.tensor(np.array(pheno.pheno[sub]), dtype=torch.float32)\n",
    "\n",
    "test_x = geno_t[sub_t]\n",
    "test_y = torch.tensor(np.array(pheno.pheno[sub_t]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff3840c-1f74-47f9-932b-70dfc5c5d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_x.contiguous(), train_y.contiguous()\n",
    "test_x, test_y = test_x.contiguous(), test_y.contiguous()\n",
    "\n",
    "train_x, train_y = train_x.to(output_device), train_y.to(output_device)\n",
    "test_x, test_y = test_x.to(output_device), test_y.to(output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d05945-e264-4ef8-986e-16ae38cc5189",
   "metadata": {},
   "source": [
    "### Test kernel partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce8bb7-648b-4adf-962f-cbde1c553027",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dfa756e-fe71-483e-8dab-6d7759e6800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, n_devices):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        base_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.MultiDeviceKernel(\n",
    "            base_covar_module, device_ids=range(n_devices),\n",
    "            output_device=output_device\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70fce0e-bb89-4a65-9c05-9c9a3ae4905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_device = torch.device('cuda:0')\n",
    "\n",
    "# n_devices = torch.cuda.device_count()\n",
    "n_devices = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f8b960d-9b8e-47b4-a89c-5c2bc59cdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(output_device)\n",
    "model = ExactGPModel(train_x, train_y, likelihood, n_devices).to(output_device)\n",
    "# model = model.to(output_device).double()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), .02)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6ef66b-953f-4ae0-b948-94b4a4ad6245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood.noise_covar.raw_noise tensor([0.], device='cuda:0')\n",
      "mean_module.constant tensor([0.], device='cuda:0')\n",
      "covar_module.module.raw_outputscale tensor(0., device='cuda:0')\n",
      "covar_module.module.base_kernel.raw_lengthscale tensor([[0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f09bc-3cf6-4632-9752-7d227625593b",
   "metadata": {},
   "source": [
    "#### Find checkpoint_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6d4ccf8-b8a1-4c54-b977-7328e7b38564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = train_x.size(0)\n",
    "\n",
    "# Find the optimum partition/checkpoint size by decreasing in powers of 2\n",
    "# Start with no partitioning (size = 0)\n",
    "settings = [0] + [int(n) for n in np.ceil(N / 2**np.arange(1, np.floor(np.log2(N))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57da8552-5782-41bc-b48b-63d8e9386422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = settings[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ea086f0-7525-4e75-9073-757d0eaeabd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def find_best_gpu_setting(train_x,\n",
    "#                           train_y,\n",
    "#                           n_devices,\n",
    "#                           output_device,\n",
    "#                           preconditioner_size\n",
    "# ):\n",
    "\n",
    "#     for checkpoint_size in settings:\n",
    "#         print('Number of devices: {} -- Kernel partition size: {}'.format(n_devices, checkpoint_size))\n",
    "#         try:\n",
    "#             # Try a full forward and backward pass with this setting to check memory usage\n",
    "            \n",
    "\n",
    "#             with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "#                  gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "#                 output = model(train_x)\n",
    "#                 loss = -mll(output, train_y)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 optimizer.step()\n",
    "    \n",
    "            \n",
    "\n",
    "#             # when successful, break out of for-loop and jump to finally block\n",
    "#             break\n",
    "#         except RuntimeError as e:\n",
    "#             print('RuntimeError: {}'.format(e))\n",
    "#         except AttributeError as e:\n",
    "#             print('AttributeError: {}'.format(e))\n",
    "#         finally:\n",
    "#             # handle CUDA OOM error\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "#     return checkpoint_size\n",
    "\n",
    "# model.train()\n",
    "# likelihood.train()\n",
    "\n",
    "# # Set a large enough preconditioner size to reduce the number of CG iterations run\n",
    "# preconditioner_size = 100\n",
    "# checkpoint_size = find_best_gpu_setting(train_x, train_y,\n",
    "#                                         n_devices=n_devices,\n",
    "#                                         output_device=output_device,\n",
    "#                                         preconditioner_size=preconditioner_size)\n",
    "# print(\"final checkpoint_size = %s\" %checkpoint_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df42084-a687-44bc-a948-067b067a62ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 29% | 23% |\n",
      "|  1 |  0% |  0% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n",
      "|  4 |  0% |  0% |\n",
      "|  5 |  0% |  0% |\n",
      "|  6 |  0% |  0% |\n",
      "|  7 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b5ab2c0-4321-4e19-b65f-061eb2353b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_part(train_x,train_y,n_devices,output_device,preconditioner_size):\n",
    "#     with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "#          gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "#         output = model(train_x)\n",
    "#         loss = -mll(output, train_y)\n",
    "#     return loss\n",
    "\n",
    "# checkpoint_size = checkpoint_size\n",
    "# preconditioner_size = 100\n",
    "\n",
    "# test_part(train_x, train_y, n_devices=n_devices, output_device=output_device, preconditioner_size=preconditioner_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522d6ac-751a-4d66-9fe0-c906ad32ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167123f-3de9-4995-9fd7-6ab1c79aecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14edcd31-3f25-4758-bbb8-a0b9ce4ea05a",
   "metadata": {},
   "source": [
    "### Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "880d906f-0a77-4675-8a48-1e86a69a7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                likelihood, \n",
    "                train_x, \n",
    "                train_y, \n",
    "                checkpoint_size, \n",
    "                preconditioner_size, \n",
    "                training_iter=300, \n",
    "                lr=.05):\n",
    "    losses = []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    for i in range(training_iter):\n",
    "        print(i)\n",
    "        with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "             gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()  \n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b0832e2-b7f1-4693-9420-3c4adacf23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.5)\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer.zero_grad()\n",
    "# Output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af7d7f7-4686-448e-b9a8-f3d529f80c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_size = train_x.shape[0]//2\n",
    "preconditioner_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bc9fac8-06be-4671-b3da-812a972993f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "#      gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "#     output = model(train_x)\n",
    "#     loss = -mll(output, train_y)\n",
    "#     loss.backward()  \n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c675d7e1-9f1d-4d12-b7ce-49d975fc730e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juannanzhou/.local/lib/python3.8/site-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:23.)\n",
      "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "losses = train_model(model, \n",
    "                likelihood, \n",
    "                train_x, \n",
    "                train_y, \n",
    "                checkpoint_size, \n",
    "                preconditioner_size, \n",
    "                training_iter=5, \n",
    "                lr=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cbf2916-93b0-4091-a0c2-add00ec04df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood.noise_covar.raw_noise tensor([-2.4149], device='cuda:0')\n",
      "mean_module.constant tensor([0.5239], device='cuda:0')\n",
      "covar_module.module.raw_outputscale tensor(-2.3764, device='cuda:0')\n",
      "covar_module.module.base_kernel.raw_lengthscale tensor([[2.3349]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1616de8a-7163-4519-b337-c428bb9e8dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 60% |\n",
      "|  1 |  0% | 48% |\n",
      "|  2 |  0% | 48% |\n",
      "|  3 |  0% | 48% |\n",
      "|  4 |  0% | 48% |\n",
      "|  5 |  0% | 48% |\n",
      "|  6 |  0% | 48% |\n",
      "|  7 |  0% | 48% |\n"
     ]
    }
   ],
   "source": [
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79020eaa-168a-4d1b-9d52-8cc8f0fe6a03",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f24f0b8-e82d-4f04-bb8b-e71080452cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.000070\n",
      "R2 = -33.943039\n",
      "mse = 0.287401\n"
     ]
    }
   ],
   "source": [
    "preconditioner_size = 100\n",
    "# checkpoint_size = checkpoint_size=train_x.shape[0]//2\n",
    "\n",
    "test_x = test_x.cuda()\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.beta_features.checkpoint_kernel(checkpoint_size):\n",
    "    # Make predictions on a small number of test points to get the test time caches computed\n",
    "    f_preds = model(test_x)\n",
    "\n",
    "\n",
    "f_mean = f_preds.mean.cpu().detach().numpy()\n",
    "y_test = test_y.detach().cpu().numpy()\n",
    "\n",
    "# epistatic\n",
    "figure(figsize=(5, 5), dpi=80)\n",
    "plt.plot(f_mean, y_test, 'o', alpha=.3)\n",
    "plt.show()\n",
    "print('r2 = %f'%pearsonr(f_mean, y_test)[0]**2)\n",
    "print('R2 = %f'%r2(y_test, f_mean))\n",
    "print('mse = %f'%mse(f_mean, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b98392-96c2-4035-8cdf-7309516a9c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f39875c-44c2-4e6a-81b7-8168f87d45a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def is_legal(v):\n",
    "    \"\"\"\n",
    "    Checks that tensor is not NaN or Inf.\n",
    "\n",
    "    Inputs:\n",
    "        v (tensor): tensor to be checked\n",
    "\n",
    "    \"\"\"\n",
    "    legal = not torch.isnan(v).any() and not torch.isinf(v)\n",
    "\n",
    "    return legal\n",
    "\n",
    "\n",
    "def polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n",
    "    \"\"\"\n",
    "    Gives the minimizer and minimum of the interpolating polynomial over given points\n",
    "    based on function and derivative information. Defaults to bisection if no critical\n",
    "    points are valid.\n",
    "\n",
    "    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n",
    "    modifications.\n",
    "\n",
    "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
    "    Last edited 12/6/18.\n",
    "\n",
    "    Inputs:\n",
    "        points (nparray): two-dimensional array with each point of form [x f g]\n",
    "        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n",
    "        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n",
    "        plot (bool): plot interpolating polynomial\n",
    "\n",
    "    Outputs:\n",
    "        x_sol (float): minimizer of interpolating polynomial\n",
    "        F_min (float): minimum of interpolating polynomial\n",
    "\n",
    "    Note:\n",
    "      . Set f or g to np.nan if they are unknown\n",
    "\n",
    "    \"\"\"\n",
    "    no_points = points.shape[0]\n",
    "    order = np.sum(1 - np.isnan(points[:, 1:3]).astype('int')) - 1\n",
    "\n",
    "    x_min = np.min(points[:, 0])\n",
    "    x_max = np.max(points[:, 0])\n",
    "\n",
    "    # compute bounds of interpolation area\n",
    "    if x_min_bound is None:\n",
    "        x_min_bound = x_min\n",
    "    if x_max_bound is None:\n",
    "        x_max_bound = x_max\n",
    "\n",
    "    # explicit formula for quadratic interpolation\n",
    "    if no_points == 2 and order == 2 and plot is False:\n",
    "        # Solution to quadratic interpolation is given by:\n",
    "        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n",
    "        # x_min = x1 - g1/(2a)\n",
    "        # if x1 = 0, then is given by:\n",
    "        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n",
    "\n",
    "        if points[0, 0] == 0:\n",
    "            x_sol = -points[0, 2] * points[1, 0] ** 2 / (2 * (points[1, 1] - points[0, 1] - points[0, 2] * points[1, 0]))\n",
    "        else:\n",
    "            a = -(points[0, 1] - points[1, 1] - points[0, 2] * (points[0, 0] - points[1, 0])) / (points[0, 0] - points[1, 0]) ** 2\n",
    "            x_sol = points[0, 0] - points[0, 2]/(2*a)\n",
    "\n",
    "        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
    "\n",
    "    # explicit formula for cubic interpolation\n",
    "    elif no_points == 2 and order == 3 and plot is False:\n",
    "        # Solution to cubic interpolation is given by:\n",
    "        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n",
    "        # d2 = sqrt(d1^2 - g1*g2)\n",
    "        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n",
    "        d1 = points[0, 2] + points[1, 2] - 3 * ((points[0, 1] - points[1, 1]) / (points[0, 0] - points[1, 0]))\n",
    "        d2 = np.sqrt(d1 ** 2 - points[0, 2] * points[1, 2])\n",
    "        if np.isreal(d2):\n",
    "            x_sol = points[1, 0] - (points[1, 0] - points[0, 0]) * ((points[1, 2] + d2 - d1) / (points[1, 2] - points[0, 2] + 2 * d2))\n",
    "            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
    "        else:\n",
    "            x_sol = (x_max_bound + x_min_bound)/2\n",
    "\n",
    "    # solve linear system\n",
    "    else:\n",
    "        # define linear constraints\n",
    "        A = np.zeros((0, order + 1))\n",
    "        b = np.zeros((0, 1))\n",
    "\n",
    "        # add linear constraints on function values\n",
    "        for i in range(no_points):\n",
    "            if not np.isnan(points[i, 1]):\n",
    "                constraint = np.zeros((1, order + 1))\n",
    "                for j in range(order, -1, -1):\n",
    "                    constraint[0, order - j] = points[i, 0] ** j\n",
    "                A = np.append(A, constraint, 0)\n",
    "                b = np.append(b, points[i, 1])\n",
    "\n",
    "        # add linear constraints on gradient values\n",
    "        for i in range(no_points):\n",
    "            if not np.isnan(points[i, 2]):\n",
    "                constraint = np.zeros((1, order + 1))\n",
    "                for j in range(order):\n",
    "                    constraint[0, j] = (order - j) * points[i, 0] ** (order - j - 1)\n",
    "                A = np.append(A, constraint, 0)\n",
    "                b = np.append(b, points[i, 2])\n",
    "\n",
    "        # check if system is solvable\n",
    "        if A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]:\n",
    "            x_sol = (x_min_bound + x_max_bound)/2\n",
    "            f_min = np.Inf\n",
    "        else:\n",
    "            # solve linear system for interpolating polynomial\n",
    "            coeff = np.linalg.solve(A, b)\n",
    "\n",
    "            # compute critical points\n",
    "            dcoeff = np.zeros(order)\n",
    "            for i in range(len(coeff) - 1):\n",
    "                dcoeff[i] = coeff[i] * (order - i)\n",
    "\n",
    "            crit_pts = np.array([x_min_bound, x_max_bound])\n",
    "            crit_pts = np.append(crit_pts, points[:, 0])\n",
    "\n",
    "            if not np.isinf(dcoeff).any():\n",
    "                roots = np.roots(dcoeff)\n",
    "                crit_pts = np.append(crit_pts, roots)\n",
    "\n",
    "            # test critical points\n",
    "            f_min = np.Inf\n",
    "            x_sol = (x_min_bound + x_max_bound) / 2 # defaults to bisection\n",
    "            for crit_pt in crit_pts:\n",
    "                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n",
    "                    F_cp = np.polyval(coeff, crit_pt)\n",
    "                    if np.isreal(F_cp) and F_cp < f_min:\n",
    "                        x_sol = np.real(crit_pt)\n",
    "                        f_min = np.real(F_cp)\n",
    "\n",
    "            if(plot):\n",
    "                plt.figure()\n",
    "                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound)/10000)\n",
    "                f = np.polyval(coeff, x)\n",
    "                plt.plot(x, f)\n",
    "                plt.plot(x_sol, f_min, 'x')\n",
    "\n",
    "    return x_sol\n",
    "\n",
    "\n",
    "class LBFGS(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n",
    "    L-BFGS implementations and (stochastic) Powell damping. Partly based on the \n",
    "    original L-BFGS implementation in PyTorch, Mark Schmidt's minFunc MATLAB code, \n",
    "    and Michael Overton's weak Wolfe line search MATLAB code.\n",
    "\n",
    "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
    "    Last edited 10/20/20.\n",
    "\n",
    "    Warnings:\n",
    "      . Does not support per-parameter options and parameter groups.\n",
    "      . All parameters have to be on a single device.\n",
    "\n",
    "    Inputs:\n",
    "        lr (float): steplength or learning rate (default: 1)\n",
    "        history_size (int): update history size (default: 10)\n",
    "        line_search (str): designates line search to use (default: 'Wolfe')\n",
    "            Options:\n",
    "                'None': uses steplength designated in algorithm\n",
    "                'Armijo': uses Armijo backtracking line search\n",
    "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
    "        dtype: data type (default: torch.float)\n",
    "        debug (bool): debugging mode\n",
    "\n",
    "    References:\n",
    "    [1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. \"A Multi-Batch L-BFGS \n",
    "        Method for Machine Learning.\" Advances in Neural Information Processing \n",
    "        Systems. 2016.\n",
    "    [2] Bollapragada, Raghu, et al. \"A Progressive Batching L-BFGS Method for Machine \n",
    "        Learning.\" International Conference on Machine Learning. 2018.\n",
    "    [3] Lewis, Adrian S., and Michael L. Overton. \"Nonsmooth Optimization via Quasi-Newton\n",
    "        Methods.\" Mathematical Programming 141.1-2 (2013): 135-163.\n",
    "    [4] Liu, Dong C., and Jorge Nocedal. \"On the Limited Memory BFGS Method for \n",
    "        Large Scale Optimization.\" Mathematical Programming 45.1-3 (1989): 503-528.\n",
    "    [5] Nocedal, Jorge. \"Updating Quasi-Newton Matrices With Limited Storage.\" \n",
    "        Mathematics of Computation 35.151 (1980): 773-782.\n",
    "    [6] Nocedal, Jorge, and Stephen J. Wright. \"Numerical Optimization.\" Springer New York,\n",
    "        2006.\n",
    "    [7] Schmidt, Mark. \"minFunc: Unconstrained Differentiable Multivariate Optimization \n",
    "        in Matlab.\" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html \n",
    "        (2005).\n",
    "    [8] Schraudolph, Nicol N., Jin Yu, and Simon Günter. \"A Stochastic Quasi-Newton \n",
    "        Method for Online Convex Optimization.\" Artificial Intelligence and Statistics. \n",
    "        2007.\n",
    "    [9] Wang, Xiao, et al. \"Stochastic Quasi-Newton Methods for Nonconvex Stochastic \n",
    "        Optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1., history_size=10, line_search='Wolfe',\n",
    "                 dtype=torch.float, debug=False):\n",
    "\n",
    "        # ensure inputs are valid\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0 <= history_size:\n",
    "            raise ValueError(\"Invalid history size: {}\".format(history_size))\n",
    "        if line_search not in ['Armijo', 'Wolfe', 'None']:\n",
    "            raise ValueError(\"Invalid line search: {}\".format(line_search))\n",
    "\n",
    "        defaults = dict(lr=lr, history_size=history_size, line_search=line_search, dtype=dtype, debug=debug)\n",
    "        super(LBFGS, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError(\"L-BFGS doesn't support per-parameter options \"\n",
    "                             \"(parameter groups)\")\n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "        self._numel_cache = None\n",
    "\n",
    "        state = self.state['global_state']\n",
    "        state.setdefault('n_iter', 0)\n",
    "        state.setdefault('curv_skips', 0)\n",
    "        state.setdefault('fail_skips', 0)\n",
    "        state.setdefault('H_diag',1)\n",
    "        state.setdefault('fail', True)\n",
    "\n",
    "        state['old_dirs'] = []\n",
    "        state['old_stps'] = []\n",
    "\n",
    "    def _numel(self):\n",
    "        if self._numel_cache is None:\n",
    "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
    "        return self._numel_cache\n",
    "\n",
    "    def _gather_flat_grad(self):\n",
    "        views = []\n",
    "        for p in self._params:\n",
    "            if p.grad is None:\n",
    "                view = p.data.new(p.data.numel()).zero_()\n",
    "            elif p.grad.data.is_sparse:\n",
    "                view = p.grad.data.to_dense().view(-1)\n",
    "            else:\n",
    "                view = p.grad.data.view(-1)\n",
    "            views.append(view)\n",
    "        return torch.cat(views, 0)\n",
    "\n",
    "    def _add_update(self, step_size, update):\n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            # view as to avoid deprecated pointwise semantics\n",
    "            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
    "            offset += numel\n",
    "        assert offset == self._numel()\n",
    "\n",
    "    def _copy_params(self):\n",
    "        current_params = []\n",
    "        for param in self._params:\n",
    "            current_params.append(deepcopy(param.data))\n",
    "        return current_params\n",
    "\n",
    "    def _load_params(self, current_params):\n",
    "        i = 0\n",
    "        for param in self._params:\n",
    "            param.data[:] = current_params[i]\n",
    "            i += 1\n",
    "\n",
    "    def line_search(self, line_search):\n",
    "        \"\"\"\n",
    "        Switches line search option.\n",
    "        \n",
    "        Inputs:\n",
    "            line_search (str): designates line search to use\n",
    "                Options:\n",
    "                    'None': uses steplength designated in algorithm\n",
    "                    'Armijo': uses Armijo backtracking line search\n",
    "                    'Wolfe': uses Armijo-Wolfe bracketing line search\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        group = self.param_groups[0]\n",
    "        group['line_search'] = line_search\n",
    "        \n",
    "        return\n",
    "\n",
    "    def two_loop_recursion(self, vec):\n",
    "        \"\"\"\n",
    "        Performs two-loop recursion on given vector to obtain Hv.\n",
    "\n",
    "        Inputs:\n",
    "            vec (tensor): 1-D tensor to apply two-loop recursion to\n",
    "\n",
    "        Output:\n",
    "            r (tensor): matrix-vector product Hv\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        history_size = group['history_size']\n",
    "\n",
    "        state = self.state['global_state']\n",
    "        old_dirs = state.get('old_dirs')    # change in gradients\n",
    "        old_stps = state.get('old_stps')    # change in iterates\n",
    "        H_diag = state.get('H_diag')\n",
    "\n",
    "        # compute the product of the inverse Hessian approximation and the gradient\n",
    "        num_old = len(old_dirs)\n",
    "\n",
    "        if 'rho' not in state:\n",
    "            state['rho'] = [None] * history_size\n",
    "            state['alpha'] = [None] * history_size\n",
    "        rho = state['rho']\n",
    "        alpha = state['alpha']\n",
    "\n",
    "        for i in range(num_old):\n",
    "            rho[i] = 1. / old_stps[i].dot(old_dirs[i])\n",
    "\n",
    "        q = vec\n",
    "        for i in range(num_old - 1, -1, -1):\n",
    "            alpha[i] = old_dirs[i].dot(q) * rho[i]\n",
    "            q.add_(-alpha[i], old_stps[i])\n",
    "\n",
    "        # multiply by initial Hessian \n",
    "        # r/d is the final direction\n",
    "        r = torch.mul(q, H_diag)\n",
    "        for i in range(num_old):\n",
    "            beta = old_stps[i].dot(r) * rho[i]\n",
    "            r.add_(alpha[i] - beta, old_dirs[i])\n",
    "\n",
    "        return r\n",
    "\n",
    "    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n",
    "        \"\"\"\n",
    "        Performs curvature update.\n",
    "\n",
    "        Inputs:\n",
    "            flat_grad (tensor): 1-D tensor of flattened gradient for computing \n",
    "                gradient difference with previously stored gradient\n",
    "            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
    "            damping (bool): flag for using Powell damping (default: False)\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(self.param_groups) == 1\n",
    "\n",
    "        # load parameters\n",
    "        if(eps <= 0):\n",
    "            raise(ValueError('Invalid eps; must be positive.'))\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        history_size = group['history_size']\n",
    "        debug = group['debug']\n",
    "\n",
    "        # variables cached in state (for tracing)\n",
    "        state = self.state['global_state']\n",
    "        fail = state.get('fail')\n",
    "        \n",
    "        # check if line search failed\n",
    "        if not fail:\n",
    "            \n",
    "            d = state.get('d')\n",
    "            t = state.get('t')\n",
    "            old_dirs = state.get('old_dirs')\n",
    "            old_stps = state.get('old_stps')\n",
    "            H_diag = state.get('H_diag')\n",
    "            prev_flat_grad = state.get('prev_flat_grad')\n",
    "            Bs = state.get('Bs')\n",
    "    \n",
    "            # compute y's\n",
    "            y = flat_grad.sub(prev_flat_grad)\n",
    "            s = d.mul(t)\n",
    "            sBs = s.dot(Bs)\n",
    "            ys = y.dot(s)  # y*s\n",
    "\n",
    "            # update L-BFGS matrix\n",
    "            if ys > eps * sBs or damping == True:\n",
    "    \n",
    "                # perform Powell damping\n",
    "                if damping == True and ys < eps*sBs:\n",
    "                    if debug:\n",
    "                        print('Applying Powell damping...')\n",
    "                    theta = ((1 - eps) * sBs)/(sBs - ys)\n",
    "                    y = theta * y + (1 - theta) * Bs\n",
    "    \n",
    "                # updating memory\n",
    "                if len(old_dirs) == history_size:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    old_dirs.pop(0)\n",
    "                    old_stps.pop(0)\n",
    "    \n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "    \n",
    "                # update scale of initial Hessian approximation\n",
    "                H_diag = ys / y.dot(y)  # (y*y)\n",
    "                \n",
    "                state['old_dirs'] = old_dirs\n",
    "                state['old_stps'] = old_stps\n",
    "                state['H_diag'] = H_diag\n",
    "\n",
    "            else:\n",
    "                # save skip\n",
    "                state['curv_skips'] += 1\n",
    "                if debug:\n",
    "                    print('Curvature pair skipped due to failed criterion')\n",
    "\n",
    "        else:\n",
    "            # save skip\n",
    "            state['fail_skips'] += 1\n",
    "            if debug:\n",
    "                print('Line search failed; curvature pair update skipped')\n",
    "\n",
    "        return\n",
    "\n",
    "    def _step(self, p_k, g_Ok, g_Sk=None, options=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Inputs:\n",
    "            p_k (tensor): 1-D tensor specifying search direction\n",
    "            g_Ok (tensor): 1-D tensor of flattened gradient over overlap O_k used\n",
    "                            for gradient differencing in curvature pair update\n",
    "            g_Sk (tensor): 1-D tensor of flattened gradient over full sample S_k\n",
    "                            used for curvature pair damping or rejection criterion,\n",
    "                            if None, will use g_Ok (default: None)\n",
    "            options (dict): contains options for performing line search (default: None)\n",
    "\n",
    "        Options for Armijo backtracking line search:\n",
    "            'closure' (callable): reevaluates model and returns function value\n",
    "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
    "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
    "            'eta' (tensor): factor for decreasing steplength > 0 (default: 2)\n",
    "            'c1' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
    "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
    "            'interpolate' (bool): flag for using interpolation (default: True)\n",
    "            'inplace' (bool): flag for inplace operations (default: True)\n",
    "            'ls_debug' (bool): debugging mode for line search\n",
    "\n",
    "        Options for Wolfe line search:\n",
    "            'closure' (callable): reevaluates model and returns function value\n",
    "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
    "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
    "            'eta' (float): factor for extrapolation (default: 2)\n",
    "            'c1' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
    "            'c2' (float): curvature condition constant in (0, 1) (default: 0.9)\n",
    "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
    "            'interpolate' (bool): flag for using interpolation (default: True)\n",
    "            'inplace' (bool): flag for inplace operations (default: True)\n",
    "            'ls_debug' (bool): debugging mode for line search\n",
    "\n",
    "        Outputs (depends on line search):\n",
    "          . No line search:\n",
    "                t (float): steplength\n",
    "          . Armijo backtracking line search:\n",
    "                F_new (tensor): loss function at new iterate\n",
    "                t (tensor): final steplength\n",
    "                ls_step (int): number of backtracks\n",
    "                closure_eval (int): number of closure evaluations\n",
    "                desc_dir (bool): descent direction flag\n",
    "                    True: p_k is descent direction with respect to the line search\n",
    "                    function\n",
    "                    False: p_k is not a descent direction with respect to the line\n",
    "                    search function\n",
    "                fail (bool): failure flag\n",
    "                    True: line search reached maximum number of iterations, failed\n",
    "                    False: line search succeeded\n",
    "          . Wolfe line search:\n",
    "                F_new (tensor): loss function at new iterate\n",
    "                g_new (tensor): gradient at new iterate\n",
    "                t (float): final steplength\n",
    "                ls_step (int): number of backtracks\n",
    "                closure_eval (int): number of closure evaluations\n",
    "                grad_eval (int): number of gradient evaluations\n",
    "                desc_dir (bool): descent direction flag\n",
    "                    True: p_k is descent direction with respect to the line search\n",
    "                    function\n",
    "                    False: p_k is not a descent direction with respect to the line\n",
    "                    search function\n",
    "                fail (bool): failure flag\n",
    "                    True: line search reached maximum number of iterations, failed\n",
    "                    False: line search succeeded\n",
    "\n",
    "        Notes:\n",
    "          . If encountering line search failure in the deterministic setting, one\n",
    "            should try increasing the maximum number of line search steps max_ls.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if options is None:\n",
    "            options = {}\n",
    "        assert len(self.param_groups) == 1\n",
    "\n",
    "        # load parameter options\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        line_search = group['line_search']\n",
    "        dtype = group['dtype']\n",
    "        debug = group['debug']\n",
    "\n",
    "        # variables cached in state (for tracing)\n",
    "        state = self.state['global_state']\n",
    "        d = state.get('d')\n",
    "        t = state.get('t')\n",
    "        prev_flat_grad = state.get('prev_flat_grad')\n",
    "        Bs = state.get('Bs')\n",
    "\n",
    "        # keep track of nb of iterations\n",
    "        state['n_iter'] += 1\n",
    "\n",
    "        # set search direction\n",
    "        d = p_k\n",
    "\n",
    "        # modify previous gradient\n",
    "        if prev_flat_grad is None:\n",
    "            prev_flat_grad = g_Ok.clone()\n",
    "        else:\n",
    "            prev_flat_grad.copy_(g_Ok)\n",
    "\n",
    "        # set initial step size\n",
    "        t = lr\n",
    "\n",
    "        # closure evaluation counter\n",
    "        closure_eval = 0\n",
    "\n",
    "        if g_Sk is None:\n",
    "            g_Sk = g_Ok.clone()\n",
    "\n",
    "        # perform Armijo backtracking line search\n",
    "        if line_search == 'Armijo':\n",
    "\n",
    "            # load options\n",
    "            if options:\n",
    "                if 'closure' not in options.keys():\n",
    "                    raise(ValueError('closure option not specified.'))\n",
    "                else:\n",
    "                    closure = options['closure']\n",
    "\n",
    "                if 'gtd' not in options.keys():\n",
    "                    gtd = g_Sk.dot(d)\n",
    "                else:\n",
    "                    gtd = options['gtd']\n",
    "\n",
    "                if 'current_loss' not in options.keys():\n",
    "                    F_k = closure()\n",
    "                    closure_eval += 1\n",
    "                else:\n",
    "                    F_k = options['current_loss']\n",
    "\n",
    "                if 'eta' not in options.keys():\n",
    "                    eta = 2\n",
    "                elif options['eta'] <= 0:\n",
    "                    raise(ValueError('Invalid eta; must be positive.'))\n",
    "                else:\n",
    "                    eta = options['eta']\n",
    "\n",
    "                if 'c1' not in options.keys():\n",
    "                    c1 = 1e-4\n",
    "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
    "                    raise(ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
    "                else:\n",
    "                    c1 = options['c1']\n",
    "\n",
    "                if 'max_ls' not in options.keys():\n",
    "                    max_ls = 10\n",
    "                elif options['max_ls'] <= 0:\n",
    "                    raise(ValueError('Invalid max_ls; must be positive.'))\n",
    "                else:\n",
    "                    max_ls = options['max_ls']\n",
    "\n",
    "                if 'interpolate' not in options.keys():\n",
    "                    interpolate = True\n",
    "                else:\n",
    "                    interpolate = options['interpolate']\n",
    "\n",
    "                if 'inplace' not in options.keys():\n",
    "                    inplace = True\n",
    "                else:\n",
    "                    inplace = options['inplace']\n",
    "                    \n",
    "                if 'ls_debug' not in options.keys():\n",
    "                    ls_debug = False\n",
    "                else:\n",
    "                    ls_debug = options['ls_debug']\n",
    "\n",
    "            else:\n",
    "                raise(ValueError('Options are not specified; need closure evaluating function.'))\n",
    "\n",
    "            # initialize values\n",
    "            if interpolate:\n",
    "                if torch.cuda.is_available():\n",
    "                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n",
    "                else:\n",
    "                    F_prev = torch.tensor(np.nan, dtype=dtype)\n",
    "\n",
    "            ls_step = 0\n",
    "            t_prev = 0 # old steplength\n",
    "            fail = False # failure flag\n",
    "\n",
    "            # begin print for debug mode\n",
    "            if ls_debug:\n",
    "                print('==================================== Begin Armijo line search ===================================')\n",
    "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
    "\n",
    "            # check if search direction is descent direction\n",
    "            if gtd >= 0:\n",
    "                desc_dir = False\n",
    "                if debug:\n",
    "                    print('Not a descent direction!')\n",
    "            else:\n",
    "                desc_dir = True\n",
    "\n",
    "            # store values if not in-place\n",
    "            if not inplace:\n",
    "                current_params = self._copy_params()\n",
    "\n",
    "            # update and evaluate at new point\n",
    "            self._add_update(t, d)\n",
    "            F_new = closure()\n",
    "            closure_eval += 1\n",
    "\n",
    "            # print info if debugging\n",
    "            if ls_debug:\n",
    "                print('LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
    "                      % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
    "\n",
    "            # check Armijo condition\n",
    "            while F_new > F_k + c1*t*gtd or not is_legal(F_new):\n",
    "\n",
    "                # check if maximum number of iterations reached\n",
    "                if ls_step >= max_ls:\n",
    "                    if inplace:\n",
    "                        self._add_update(-t, d)\n",
    "                    else:\n",
    "                        self._load_params(current_params)\n",
    "\n",
    "                    t = 0\n",
    "                    F_new = closure()\n",
    "                    closure_eval += 1\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # store current steplength\n",
    "                    t_new = t\n",
    "\n",
    "                    # compute new steplength\n",
    "\n",
    "                    # if first step or not interpolating, then multiply by factor\n",
    "                    if ls_step == 0 or not interpolate or not is_legal(F_new):\n",
    "                        t = t/eta\n",
    "\n",
    "                    # if second step, use function value at new point along with \n",
    "                    # gradient and function at current iterate\n",
    "                    elif ls_step == 1 or not is_legal(F_prev):\n",
    "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n",
    "\n",
    "                    # otherwise, use function values at new point, previous point,\n",
    "                    # and gradient and function at current iterate\n",
    "                    else:\n",
    "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan], \n",
    "                                                [t_prev, F_prev.item(), np.nan]]))\n",
    "\n",
    "                    # if values are too extreme, adjust t\n",
    "                    if interpolate:\n",
    "                        if t < 1e-3 * t_new:\n",
    "                            t = 1e-3 * t_new\n",
    "                        elif t > 0.6 * t_new:\n",
    "                            t = 0.6 * t_new\n",
    "\n",
    "                        # store old point\n",
    "                        F_prev = F_new\n",
    "                        t_prev = t_new\n",
    "\n",
    "                    # update iterate and reevaluate\n",
    "                    if inplace:\n",
    "                        self._add_update(t - t_new, d)\n",
    "                    else:\n",
    "                        self._load_params(current_params)\n",
    "                        self._add_update(t, d)\n",
    "\n",
    "                    F_new = closure()\n",
    "                    closure_eval += 1\n",
    "                    ls_step += 1 # iterate\n",
    "                    \n",
    "                    # print info if debugging\n",
    "                    if ls_debug:\n",
    "                        print('LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
    "                              % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
    "\n",
    "            # store Bs\n",
    "            if Bs is None:\n",
    "                Bs = (g_Sk.mul(-t)).clone()\n",
    "            else:\n",
    "                Bs.copy_(g_Sk.mul(-t))\n",
    "                \n",
    "            # print final steplength\n",
    "            if ls_debug:\n",
    "                print('Final Steplength:', t)\n",
    "                print('===================================== End Armijo line search ====================================')\n",
    "\n",
    "            state['d'] = d\n",
    "            state['prev_flat_grad'] = prev_flat_grad\n",
    "            state['t'] = t\n",
    "            state['Bs'] = Bs\n",
    "            state['fail'] = fail\n",
    "\n",
    "            return F_new, t, ls_step, closure_eval, desc_dir, fail\n",
    "\n",
    "        # perform weak Wolfe line search\n",
    "        elif line_search == 'Wolfe':\n",
    "\n",
    "            # load options\n",
    "            if options:\n",
    "                if 'closure' not in options.keys():\n",
    "                    raise(ValueError('closure option not specified.'))\n",
    "                else:\n",
    "                    closure = options['closure']\n",
    "\n",
    "                if 'current_loss' not in options.keys():\n",
    "                    F_k = closure()\n",
    "                    closure_eval += 1\n",
    "                else:\n",
    "                    F_k = options['current_loss']\n",
    "\n",
    "                if 'gtd' not in options.keys():\n",
    "                    gtd = g_Sk.dot(d)\n",
    "                else:\n",
    "                    gtd = options['gtd']\n",
    "\n",
    "                if 'eta' not in options.keys():\n",
    "                    eta = 2\n",
    "                elif options['eta'] <= 1:\n",
    "                    raise(ValueError('Invalid eta; must be greater than 1.'))\n",
    "                else:\n",
    "                    eta = options['eta']\n",
    "\n",
    "                if 'c1' not in options.keys():\n",
    "                    c1 = 1e-4\n",
    "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
    "                    raise(ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
    "                else:\n",
    "                    c1 = options['c1']\n",
    "\n",
    "                if 'c2' not in options.keys():\n",
    "                    c2 = 0.9\n",
    "                elif options['c2'] >= 1 or options['c2'] <= 0:\n",
    "                    raise(ValueError('Invalid c2; must be strictly between 0 and 1.'))\n",
    "                elif options['c2'] <= c1:\n",
    "                    raise(ValueError('Invalid c2; must be strictly larger than c1.'))\n",
    "                else:\n",
    "                    c2 = options['c2']\n",
    "\n",
    "                if 'max_ls' not in options.keys():\n",
    "                    max_ls = 10\n",
    "                elif options['max_ls'] <= 0:\n",
    "                    raise(ValueError('Invalid max_ls; must be positive.'))\n",
    "                else:\n",
    "                    max_ls = options['max_ls']\n",
    "\n",
    "                if 'interpolate' not in options.keys():\n",
    "                    interpolate = True\n",
    "                else:\n",
    "                    interpolate = options['interpolate']\n",
    "\n",
    "                if 'inplace' not in options.keys():\n",
    "                    inplace = True\n",
    "                else:\n",
    "                    inplace = options['inplace']\n",
    "                    \n",
    "                if 'ls_debug' not in options.keys():\n",
    "                    ls_debug = False\n",
    "                else:\n",
    "                    ls_debug = options['ls_debug']\n",
    "\n",
    "            else:\n",
    "                raise(ValueError('Options are not specified; need closure evaluating function.'))\n",
    "\n",
    "            # initialize counters\n",
    "            ls_step = 0\n",
    "            grad_eval = 0 # tracks gradient evaluations\n",
    "            t_prev = 0 # old steplength\n",
    "\n",
    "            # initialize bracketing variables and flag\n",
    "            alpha = 0\n",
    "            beta = float('Inf')\n",
    "            fail = False\n",
    "\n",
    "            # initialize values for line search\n",
    "            if(interpolate):\n",
    "                F_a = F_k\n",
    "                g_a = gtd\n",
    "\n",
    "                if(torch.cuda.is_available()):\n",
    "                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
    "                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
    "                else:\n",
    "                    F_b = torch.tensor(np.nan, dtype=dtype)\n",
    "                    g_b = torch.tensor(np.nan, dtype=dtype)\n",
    "\n",
    "            # begin print for debug mode\n",
    "            if ls_debug:\n",
    "                print('==================================== Begin Wolfe line search ====================================')\n",
    "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
    "\n",
    "            # check if search direction is descent direction\n",
    "            if gtd >= 0:\n",
    "                desc_dir = False\n",
    "                if debug:\n",
    "                    print('Not a descent direction!')\n",
    "            else:\n",
    "                desc_dir = True\n",
    "\n",
    "            # store values if not in-place\n",
    "            if not inplace:\n",
    "                current_params = self._copy_params()\n",
    "\n",
    "            # update and evaluate at new point\n",
    "            self._add_update(t, d)\n",
    "            F_new = closure()\n",
    "            closure_eval += 1\n",
    "\n",
    "            # main loop\n",
    "            while True:\n",
    "\n",
    "                # check if maximum number of line search steps have been reached\n",
    "                if ls_step >= max_ls:\n",
    "                    if inplace:\n",
    "                        self._add_update(-t, d)\n",
    "                    else:\n",
    "                        self._load_params(current_params)\n",
    "\n",
    "                    t = 0\n",
    "                    F_new = closure()\n",
    "                    F_new.backward()\n",
    "                    g_new = self._gather_flat_grad()\n",
    "                    closure_eval += 1\n",
    "                    grad_eval += 1\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                # print info if debugging\n",
    "                if ls_debug:\n",
    "                    print('LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e' \n",
    "                          % (ls_step, t, alpha, beta))\n",
    "                    print('Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
    "                          % (F_new, F_k + c1 * t * gtd, F_k))\n",
    "\n",
    "                # check Armijo condition\n",
    "                if F_new > F_k + c1 * t * gtd:\n",
    "\n",
    "                    # set upper bound\n",
    "                    beta = t\n",
    "                    t_prev = t\n",
    "\n",
    "                    # update interpolation quantities\n",
    "                    if interpolate:\n",
    "                        F_b = F_new\n",
    "                        if torch.cuda.is_available():\n",
    "                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
    "                        else:\n",
    "                            g_b = torch.tensor(np.nan, dtype=dtype)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # compute gradient\n",
    "                    F_new.backward()\n",
    "                    g_new = self._gather_flat_grad()\n",
    "                    grad_eval += 1\n",
    "                    gtd_new = g_new.dot(d)\n",
    "                    \n",
    "                    # print info if debugging\n",
    "                    if ls_debug:\n",
    "                        print('Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e'\n",
    "                              % (gtd_new, c2 * gtd, gtd))\n",
    "\n",
    "                    # check curvature condition\n",
    "                    if gtd_new < c2 * gtd:\n",
    "\n",
    "                        # set lower bound\n",
    "                        alpha = t\n",
    "                        t_prev = t\n",
    "\n",
    "                        # update interpolation quantities\n",
    "                        if interpolate:\n",
    "                            F_a = F_new\n",
    "                            g_a = gtd_new\n",
    "\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # compute new steplength\n",
    "\n",
    "                # if first step or not interpolating, then bisect or multiply by factor\n",
    "                if not interpolate or not is_legal(F_b):\n",
    "                    if beta == float('Inf'):\n",
    "                        t = eta*t\n",
    "                    else:\n",
    "                        t = (alpha + beta)/2.0\n",
    "\n",
    "                # otherwise interpolate between a and b\n",
    "                else:\n",
    "                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()], [beta, F_b.item(), g_b.item()]]))\n",
    "\n",
    "                    # if values are too extreme, adjust t\n",
    "                    if beta == float('Inf'):\n",
    "                        if t > 2 * eta * t_prev:\n",
    "                            t = 2 * eta * t_prev\n",
    "                        elif t < eta * t_prev:\n",
    "                            t = eta * t_prev\n",
    "                    else:\n",
    "                        if t < alpha + 0.2 * (beta - alpha):\n",
    "                            t = alpha + 0.2 * (beta - alpha)\n",
    "                        elif t > (beta - alpha) / 2.0:\n",
    "                            t = (beta - alpha) / 2.0\n",
    "\n",
    "                    # if we obtain nonsensical value from interpolation\n",
    "                    if t <= 0:\n",
    "                        t = (beta - alpha) / 2.0\n",
    "\n",
    "                # update parameters\n",
    "                if inplace:\n",
    "                    self._add_update(t - t_prev, d)\n",
    "                else:\n",
    "                    self._load_params(current_params)\n",
    "                    self._add_update(t, d)\n",
    "\n",
    "                # evaluate closure\n",
    "                F_new = closure()\n",
    "                closure_eval += 1\n",
    "                ls_step += 1\n",
    "\n",
    "            # store Bs\n",
    "            if Bs is None:\n",
    "                Bs = (g_Sk.mul(-t)).clone()\n",
    "            else:\n",
    "                Bs.copy_(g_Sk.mul(-t))\n",
    "                \n",
    "            # print final steplength\n",
    "            if ls_debug:\n",
    "                print('Final Steplength:', t)\n",
    "                print('===================================== End Wolfe line search =====================================')\n",
    "\n",
    "            state['d'] = d\n",
    "            state['prev_flat_grad'] = prev_flat_grad\n",
    "            state['t'] = t\n",
    "            state['Bs'] = Bs\n",
    "            state['fail'] = fail\n",
    "\n",
    "            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n",
    "\n",
    "        else:\n",
    "\n",
    "            # perform update\n",
    "            self._add_update(t, d)\n",
    "\n",
    "            # store Bs\n",
    "            if Bs is None:\n",
    "                Bs = (g_Sk.mul(-t)).clone()\n",
    "            else:\n",
    "                Bs.copy_(g_Sk.mul(-t))\n",
    "\n",
    "            state['d'] = d\n",
    "            state['prev_flat_grad'] = prev_flat_grad\n",
    "            state['t'] = t\n",
    "            state['Bs'] = Bs\n",
    "            state['fail'] = False\n",
    "\n",
    "            return t\n",
    "        \n",
    "    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n",
    "        return self._step(p_k, g_Ok, g_Sk, options)\n",
    "\n",
    "\n",
    "class FullBatchLBFGS(LBFGS):\n",
    "    \"\"\"\n",
    "    Implements full-batch or deterministic L-BFGS algorithm. Compatible with\n",
    "    Powell damping. Can be used when evaluating a deterministic function and\n",
    "    gradient. Wraps the LBFGS optimizer. Performs the two-loop recursion,\n",
    "    updating, and curvature updating in a single step.\n",
    "\n",
    "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
    "    Last edited 11/15/18.\n",
    "\n",
    "    Warnings:\n",
    "      . Does not support per-parameter options and parameter groups.\n",
    "      . All parameters have to be on a single device.\n",
    "\n",
    "    Inputs:\n",
    "        lr (float): steplength or learning rate (default: 1)\n",
    "        history_size (int): update history size (default: 10)\n",
    "        line_search (str): designates line search to use (default: 'Wolfe')\n",
    "            Options:\n",
    "                'None': uses steplength designated in algorithm\n",
    "                'Armijo': uses Armijo backtracking line search\n",
    "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
    "        dtype: data type (default: torch.float)\n",
    "        debug (bool): debugging mode\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1, history_size=10, line_search='Wolfe', \n",
    "                 dtype=torch.float, debug=False):\n",
    "        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search, \n",
    "             dtype, debug)\n",
    "\n",
    "    def step(self, options=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Inputs:\n",
    "            options (dict): contains options for performing line search (default: None)\n",
    "            \n",
    "        General Options:\n",
    "            'eps' (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
    "            'damping' (bool): flag for using Powell damping (default: False)\n",
    "\n",
    "        Options for Armijo backtracking line search:\n",
    "            'closure' (callable): reevaluates model and returns function value\n",
    "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
    "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
    "            'eta' (tensor): factor for decreasing steplength > 0 (default: 2)\n",
    "            'c1' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
    "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
    "            'interpolate' (bool): flag for using interpolation (default: True)\n",
    "            'inplace' (bool): flag for inplace operations (default: True)\n",
    "            'ls_debug' (bool): debugging mode for line search\n",
    "\n",
    "        Options for Wolfe line search:\n",
    "            'closure' (callable): reevaluates model and returns function value\n",
    "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
    "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
    "            'eta' (float): factor for extrapolation (default: 2)\n",
    "            'c1' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
    "            'c2' (float): curvature condition constant in (0, 1) (default: 0.9)\n",
    "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
    "            'interpolate' (bool): flag for using interpolation (default: True)\n",
    "            'inplace' (bool): flag for inplace operations (default: True)\n",
    "            'ls_debug' (bool): debugging mode for line search\n",
    "\n",
    "        Outputs (depends on line search):\n",
    "          . No line search:\n",
    "                t (float): steplength\n",
    "          . Armijo backtracking line search:\n",
    "                F_new (tensor): loss function at new iterate\n",
    "                t (tensor): final steplength\n",
    "                ls_step (int): number of backtracks\n",
    "                closure_eval (int): number of closure evaluations\n",
    "                desc_dir (bool): descent direction flag\n",
    "                    True: p_k is descent direction with respect to the line search\n",
    "                    function\n",
    "                    False: p_k is not a descent direction with respect to the line\n",
    "                    search function\n",
    "                fail (bool): failure flag\n",
    "                    True: line search reached maximum number of iterations, failed\n",
    "                    False: line search succeeded\n",
    "          . Wolfe line search:\n",
    "                F_new (tensor): loss function at new iterate\n",
    "                g_new (tensor): gradient at new iterate\n",
    "                t (float): final steplength\n",
    "                ls_step (int): number of backtracks\n",
    "                closure_eval (int): number of closure evaluations\n",
    "                grad_eval (int): number of gradient evaluations\n",
    "                desc_dir (bool): descent direction flag\n",
    "                    True: p_k is descent direction with respect to the line search\n",
    "                    function\n",
    "                    False: p_k is not a descent direction with respect to the line\n",
    "                    search function\n",
    "                fail (bool): failure flag\n",
    "                    True: line search reached maximum number of iterations, failed\n",
    "                    False: line search succeeded\n",
    "\n",
    "        Notes:\n",
    "          . If encountering line search failure in the deterministic setting, one\n",
    "            should try increasing the maximum number of line search steps max_ls.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # load options for damping and eps\n",
    "        if 'damping' not in options.keys():\n",
    "            damping = False\n",
    "        else:\n",
    "            damping = options['damping']\n",
    "            \n",
    "        if 'eps' not in options.keys():\n",
    "            eps = 1e-2\n",
    "        else:\n",
    "            eps = options['eps']\n",
    "        \n",
    "        # gather gradient\n",
    "        grad = self._gather_flat_grad()\n",
    "        \n",
    "        # update curvature if after 1st iteration\n",
    "        state = self.state['global_state']\n",
    "        if state['n_iter'] > 0:\n",
    "            self.curvature_update(grad, eps, damping)\n",
    "\n",
    "        # compute search direction\n",
    "        p = self.two_loop_recursion(-grad)\n",
    "\n",
    "        # take step\n",
    "        return self._step(p, grad, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6c4ba-8779-4519-812d-9158e825d1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e295fa-d54e-4df9-b021-6c8faf267946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3d22e-5eff-4a91-91f2-636efd0510c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab2830-e78a-42ad-a1e3-570f93ed4ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67351d-35c3-4a1c-91dc-d324b9465898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312dd00-3522-4483-b7f7-d4494edbe050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24ab30-1b56-43ac-9ac6-c7cf9d014a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad83e96-7ca9-4500-b403-2ac352d61216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66186cfe-74d9-488c-ab60-1f38b37997cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ff8b3-d8b4-41c9-b3e6-26620fb24bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab7dc0-cd50-4d39-a836-35623657a6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf03275-d579-4ba4-987f-dca704f92766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
